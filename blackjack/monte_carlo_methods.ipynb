{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from operator import getitem, setitem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Generally speaking Monte Carlo control is characterised by approximating the value function of state action pairs by using sample approximations for their rewards.\n",
    "\n",
    "In particular the Monte Carlo approach doesn't bootstrap the value function but instead observes a full episode before "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the value function\n",
    "\n",
    "A psuedo algorithm for learning the value function using monte carlo methods is roughly;  \n",
    "\n",
    "- Initialise\n",
    "- while episode running:\n",
    "    - take a step using policy and observe S0\n",
    "    - for each step:\n",
    "        - rewards = gamma*rewards + return\n",
    "        - append state_history {state: rewards}\n",
    "        - value[state] = mean(state_history[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key nugget here is that the monte carlo approach can only reflect on the merits of its decisions once an episode has finished and the states observed. Unless of course the agent visits a single state more than once within the same episode.\n",
    "\n",
    "There are some merits to updating the value function as often as possible when running many agents concurrently in async while updating the same value functions.\n",
    "\n",
    "Key traits:  \n",
    "- the value function is not used when estimating improvements, we only average past experience  \n",
    "- mostly assumes the statespace is of reasonable size and discrete (as it actively stores state values, atleast in its simplest form)  \n",
    "- assumes a soft policy function with coverage, that is enters all states infinitely many times \n",
    "- gamma can be set to 1/n if we whish to use the sample mean (X bar) estimate for state rewards, gamma [0,1) fits non-stationary distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining some basic helper objects;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scorer:\n",
    "    \n",
    "    def __init__(self,score = 0, gamma=0.8):\n",
    "        self.score = score\n",
    "        self.gamma = gamma\n",
    "    def observe(self,reward, gamma=None):\n",
    "        # when controlling learning rate stochastically (e.g. anealing)\n",
    "        if gamma:\n",
    "            self.score = self.score * gamma + reward\n",
    "        else:\n",
    "            self.score = self.score * self.gamma + reward\n",
    "        \n",
    "    def show_score(self):\n",
    "        return self.score\n",
    "    \n",
    "    def reset():\n",
    "        self.score = 0\n",
    " \n",
    "class MCValueFunction:\n",
    "    def __init__(self):\n",
    "        self.history = defaultdict(list)\n",
    "        \n",
    "    def update(self,observation, action , score):\n",
    "        \n",
    "        # to support arbitrarily nested observations we will loop through them\n",
    "        reduce(setitem, observation, self.history)\n",
    "                \n",
    "#         np.append(self.history[observation][action],score)\n",
    "        \n",
    "    def predict(self,observation, action):\n",
    "        \n",
    "        # take observation depth (tuple) and average its list of rewards\n",
    "        value = np.mean(reduce(getitem, observation, self.history))\n",
    "#         value = np.mean(self.history[observation][action])\n",
    "           \n",
    "        # if the list is empty, 0 the nan value\n",
    "        value = 0 if np.isnan(value) else value\n",
    "        \n",
    "        return value\n",
    "    \n",
    "class GreedyPolicy:\n",
    "    def __init__():\n",
    "         pass\n",
    "\n",
    "    def choose(observation, actions, value_function):\n",
    "        values = np.array([value_function.predict(observation, action) for action in actions])\n",
    "        greedy_choice = np.argmax(values)\n",
    "        \n",
    "        return actions[greedy_choice]\n",
    "\n",
    "class RandomPolicy:\n",
    "    def __init__(self):\n",
    "         pass\n",
    "        \n",
    "    def choose(env):\n",
    "        return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BlackJack value function\n",
    "\n",
    "To learn a value function of the blackjack game we need to learn state action pair values, because we don't have a reliable model for the states those actions will lead to.  \n",
    "If we did we could simply infer;  \n",
    "    new_state(action) = model(current_state, action)  \n",
    "    value = [value[new_state] for action in action_space]  \n",
    "    \n",
    "To learn the state-action values we will need to explore all the posibilities. If we don't manually set a unique starting point for the game we must use a soft policy that gaurentees coverage of all states. The simplest way to do this is using a random choice policy. With enough episodes we are gaurenteed to visit every state-action pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 10)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "(9, 10, False)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-b84862aa965f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mscorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mvalue_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-bb2496fc1ee7>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, observation, action, score)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# initialize arrays if empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mobservation\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (9, 10, False)"
     ]
    }
   ],
   "source": [
    "env = gym.make('Blackjack-v0')\n",
    "\n",
    "episodes = 100\n",
    "gamma = 0.8\n",
    "\n",
    "scorer = Scorer(gamma=gamma)\n",
    "policy = RandomPolicy()\n",
    "value_function = MCValueFunction()\n",
    "\n",
    "observation = env.reset()\n",
    "for _ in range(episodes):\n",
    "    \n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    scorer.observe(reward)\n",
    "    value_function.update(observation, action , scorer.show_score())\n",
    "    \n",
    "    if done:\n",
    "        scorer.reset()\n",
    "        observation = env.reset()\n",
    "        print('episode ends')\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 4, False)\n",
      "-1.0\n",
      "episode ends\n",
      "(14, 6, True)\n",
      "1.0\n",
      "episode ends\n",
      "(15, 3, False)\n",
      "-1.0\n",
      "episode ends\n",
      "(21, 6, True)\n",
      "-1.0\n",
      "(15, 6, False)\n",
      "0.0\n",
      "episode ends\n",
      "(11, 9, False)\n",
      "1.0\n",
      "episode ends\n",
      "(21, 4, True)\n",
      "1.0\n",
      "(21, 4, False)\n",
      "0.0\n",
      "episode ends\n",
      "(18, 10, False)\n",
      "-1.0\n",
      "episode ends\n",
      "(20, 6, False)\n",
      "-1.0\n",
      "episode ends\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Blackjack-v0')\n",
    "observation = env.reset()\n",
    "for _ in range(10):\n",
    "    print(observation), print(reward)\n",
    "    \n",
    "#     env.render() #no render implementation for blackjack\n",
    "    action = env.action_space.sample() # your agent here (this takes random actions)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        observation = env.reset()\n",
    "        print('episode ends')\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "empty() missing required argument 'shape' (pos 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d18d581e1fd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: empty() missing required argument 'shape' (pos 1)"
     ]
    }
   ],
   "source": [
    "np.empty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.mean([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0 if np.isnan(x) else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
