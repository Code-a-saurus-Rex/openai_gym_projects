{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b57e7188-aa0d-484b-8fa1-3e2473e9cd79",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Finite Markov Decision Processes (MDPs)\n",
    "\n",
    "In this topic we will discuss a more general concept of learning or control in an environment. Next we will cover practical methods for implementing policies and learning value functions.\n",
    "\n",
    "A Markov Decision Process (MDP) is a mathematical framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision maker. An MDP consists of a set of states, a set of actions, a set of rewards, and a set of transition probabilities. At each time step, the agent observes the current state, selects an action, receives a reward, and transitions to a new state according to the transition probabilities.\n",
    "\n",
    "A finite MDP is an MDP with a finite set of states and actions. In a finite MDP, it is possible to compute the optimal policy (i.e., the policy that maximizes the expected cumulative reward) using the Bellman equations.\n",
    "\n",
    "A Markov Decision Process (MDP) is a more general framework than an epsilon-greedy bandit problem because it allows for stochastic transitions between states and the ability to choose actions that affect future rewards. In an epsilon-greedy bandit problem, the agent only needs to choose between a finite set of actions and does not have to worry about the state of the environment. In contrast, an MDP allows the agent to take actions that can influence the state of the environment and affect future rewards. This makes MDPs a more general and powerful framework for modeling and solving a wide range of decision-making problems.\n",
    "\n",
    "**The Agent-Environment Interface**\n",
    "\n",
    "In reinforcement learning, an agent interacts with an environment to learn how to make decisions that maximize a reward signal. The agent and environment interact at each time step, with the agent observing the current state of the environment, selecting an action to perform, and receiving a reward from the environment. The environment then transitions to a new state, and the process repeats.\n",
    "\n",
    "The agent's goal is to learn a policy that maximizes the expected cumulative reward over time. The policy is a mapping from states to actions, and it specifies what action the agent should take in each state. The optimal policy is the policy that maximizes the expected cumulative reward.\n",
    "\n",
    "The agent-environment interface is a key concept in reinforcement learning, as it provides a general framework for modeling a wide range of decision-making problems. By defining the states, actions, and rewards of an environment, we can use reinforcement learning algorithms to learn how to make optimal decisions in that environment.\n",
    "\n",
    "The state-transition probabilities for a finite Markov Decision Process (MDP) can be represented as a set of matrices `P_a` for each action `a`, where `P_a(s, s')` is the probability of transitioning from state `s` to state `s'` after taking action `a`. These matrices satisfy the Markov property, which states that the probability of transitioning to a new state `s'` only depends on the current state `s` and the action taken `a`, and not on the history of previous states and actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178fde42-dcf6-4137-8cd4-7ec9dd870e75",
   "metadata": {},
   "source": [
    "![](https://cfml.se/img/blog/markov_decision_processes/rl_an_introduction_fig_3_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ebcd0f-4d3a-4e37-9038-ecafd2072e9c",
   "metadata": {},
   "source": [
    "_all images pulled directly from the internet, I take no credit_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6f2120-620d-4d02-98c8-7590fec28d4e",
   "metadata": {
    "tags": []
   },
   "source": [
    "The expected reward for any state-action pair can be calculated using the following equation:\n",
    "\n",
    "`E[R | s, a] = Î£_s' Î£_r P(s', r | s, a) * r`\n",
    "\n",
    "where:\n",
    "\n",
    "- `E[R | s, a]` is the expected reward for taking action `a` in state `s`\n",
    "- `Î£_s'` is the sum over all possible next states `s'`\n",
    "- `Î£_r` is the sum over all possible rewards `r`\n",
    "- `P(s', r | s, a)` is the probability of transitioning to state `s'` and receiving reward `r` after taking action `a` in state `s`\n",
    "\n",
    "This equation computes the expected reward that the agent will receive after taking action `a` in state `s`. It takes into account all possible outcomes of the action, including the probability of transitioning to each possible next state and receiving each possible reward. This expected reward can be used to evaluate the quality of different actions in a given state, and to estimate the optimal policy for an agent in a given MDP.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed8060a-fd13-490e-9c75-b2566820ff95",
   "metadata": {},
   "source": [
    "## Cumulative rewards\n",
    "\n",
    "The cumulative reward, denoted as `R`, is the sum of all the rewards obtained by the agent over a series of time steps, and can be expressed as:\n",
    "\n",
    "`R = Î£_t Î³^t r_t`\n",
    "\n",
    "The first few terms of the cumulative reward equation `R = Î£_t Î³^t r_t` can be written as `r_0 + Î³r_1 + Î³^2r_2 + ... + Î³^tr_t`. The discount factor `Î³` determines the importance of immediate and future rewards, and the sequence represents the sum of rewards obtained by the agent over a series of time steps.\n",
    "\n",
    "where:\n",
    "\n",
    "- `Î³` is the discount factor, which determines the relative importance of immediate and future rewards.\n",
    "- `r_t` is the reward obtained by the agent at time step `t`.\n",
    "\n",
    "This equation is used to evaluate the quality of different policies in an MDP. The goal is to find the policy that maximizes the expected cumulative reward over time.\n",
    "\n",
    "By applying a discount factor we can incentivize the agent to find optimal strategies, for example in solving a maze an agent would score better for using the shortest route. It also ensures that the total rewards remain bounded.\n",
    "\n",
    "### What we have learned\n",
    "\n",
    "1. Estimated next reward\n",
    "\n",
    "The estimate for the next reward given a state and action is the sum of the rewards possible for each future state and the sum over all possible states times the probability of reaching that state from the previous action\n",
    "\n",
    "1. Cumulative rewards\n",
    "\n",
    "We add up rewards as our agent navigates the environment while applying a discounting factor\n",
    "\n",
    "### How do we approach selecting actions?\n",
    "\n",
    "Simply knowing the expected reward and how to calculate cumulative rewards does not give us a concise equation for picking actions.\n",
    "\n",
    "We need to consider the following:\n",
    "\n",
    "- Explore vs exploit\n",
    "- Maximizing future rewards, not just immediate rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dda2e53-1439-4978-826c-9047f4983bf4",
   "metadata": {},
   "source": [
    "## Policies and Value Functions\n",
    "\n",
    "In the context of MDPs, the `policy` is a mapping from states to actions, and it specifies what action the agent should take in each state. The goal of the agent is to learn an optimal policy that maximizes the expected cumulative reward over time. The policy can be represented as a function, denoted as `Ï€(s)`, that maps each state `s` to an action `a`. The value of `Ï€(s)` denotes the action that the agent should take in state `s` according to the policy. The optimal policy, denoted as `Ï€*`, is the policy that maximizes the expected cumulative reward over time for all states. The optimal policy can be computed using the Bellman equations, which describe the relationship between the value function and the policy in an MDP.\n",
    "\n",
    "- `Ï€*`, optimal policy\n",
    "- `Ï€(s)`, a current policy mapping an action to state `s`\n",
    "\n",
    "In the context of MDPs, the `value function` is a function that assigns a value to each state or state-action pair, representing the expected cumulative reward that the agent can obtain starting from that state (or state-action pair) and following the policy. The value function can be used to evaluate the quality of different policies and to compute the optimal policy for an MDP. \n",
    "\n",
    "The optimal value function, denoted as `V*`, maximizes the expected returns in any state `s` if we continue acting according to the optimal policy `Ï€*` .\n",
    "\n",
    "The policy function is said to be optimal because it is acting according to an optimal value function.\n",
    "\n",
    "Said in a different way the optimal value function is the current estimate of the value of rewards we will likely receive in the future (discounted to the current point in time) because we are in state `s` if we continue to pick actions according to our optimal policy.\n",
    "\n",
    "- ðŸ’¡ `V*`, optimal value function is expected cumulative rewards under optimal policy\n",
    "\n",
    "The value function for a state `s` over a policy `Ï€`, denoted as `V_Ï€(s)`, is the expected cumulative reward that the agent can obtain starting from state `s` and following policy `Ï€`. It can be expressed in terms of the state-transition probabilities `P(s', r | s, a)` and the expected reward `E[R | s, a]` as follows:\n",
    "\n",
    "`V_Ï€(s) = Î£_a Ï€(a | s) * (E[R | s, a] + Î³ * Î£_s' P(s' | s, a) * V_Ï€(s'))`\n",
    "\n",
    "where:\n",
    "\n",
    "- `Î£_a` is the sum over all possible actions `a`\n",
    "- `Ï€(a | s)` is the probability of taking action `a` in state `s` according to policy `Ï€`\n",
    "- `E[R | s, a]` is the expected reward for taking action `a` in state `s`\n",
    "- `Î³` is the discount factor, which determines the relative importance of immediate and future rewards.\n",
    "- `Î£_s'` is the sum over all possible next states `s'`\n",
    "- `P(s' | s, a)` is the probability of transitioning to state `s'` after taking action `a` in state `s`\n",
    "- `V_Ï€(s')` is the value function for the next state `s'` under policy `Ï€`\n",
    "\n",
    "If we substitute the equations we know:\n",
    "\n",
    "`V_Ï€(s) = Î£_a Ï€(a | s) * (Î£_s' Î£_r P(s', r | s, a) * r + Î³ * Î£_s' Î£_r P(s', r | s, a) * V_Ï€(s'))`\n",
    "\n",
    "we can simplify the common expression to get the more generally used:\n",
    "\n",
    "`V_Ï€(s) = Î£_a Ï€(a | s) * (Î£_s' Î£_r P(s', r | s, a) * [ r + Î³ * V_Ï€(s'))`\n",
    "\n",
    "- `Q*(s, a)`, optimal action value function\n",
    "\n",
    "The optimal action value function, denoted as `Q*(s, a)`, is the expected cumulative reward that the agent can obtain starting from state `s`, taking action `a`, and following the optimal policy thereafter. It can be expressed in terms of the optimal value function `V*(s)` as follows:\n",
    "\n",
    "`Q*(s, a) = Î£_s' P(s', r | s, a) * (r + Î³ * V*(s'))`\n",
    "\n",
    "where:\n",
    "\n",
    "- `Î£_s'` is the sum over all possible next states `s'`\n",
    "- `P(s', r | s, a)` is the probability of transitioning to state `s'` and receiving reward `r` after taking action `a` in state `s`\n",
    "- `r` is the reward obtained by the agent after taking action `a` in state `s`\n",
    "- `Î³` is the discount factor, which determines the relative importance of immediate and future rewards.\n",
    "- `V*(s')` is the optimal value function for the next state `s'`\n",
    "\n",
    "This equation computes the expected cumulative reward that the agent can obtain by taking action `a` in state `s`, transitioning to a new state `s'` with probability `P(s', r | s, a)`, receiving a reward `r`, and then following the optimal policy thereafter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18de015-b881-4127-8512-78cbff8c34d4",
   "metadata": {},
   "source": [
    "## Bellman equations\n",
    "\n",
    "Now we will show that what we have derived above is simply the notorious Bellman equations\n",
    "\n",
    "**State-value Bellman equation:**\n",
    "\n",
    "`V(s) = Î£_a Ï€(a|s) Î£_s' P(s'|s,a) [R(s,a,s') + Î³V(s')]`\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "- `V(s)` is the expected cumulative reward starting from state `s` under policy `Ï€`\n",
    "- `Ï€(a|s)` is the probability of taking action `a` in state `s` under policy `Ï€`\n",
    "- `P(s'|s,a)` is the probability of transitioning to state `s'` after taking action `a` in state `s`\n",
    "- `R(s,a,s')` is the reward received for transitioning from state `s` to state `s'` after taking action `a`\n",
    "- `Î³` is the discount factor, which determines the relative importance of immediate and future rewards.\n",
    "\n",
    "This equation states that the value of a state `s` under policy `Ï€` is equal to the expected immediate reward of taking an action under `Ï€` in state `s`, plus the expected discounted future reward of transitioning to a successor state `s'`, weighted by the probability of taking that action and transitioning to that state.\n",
    "\n",
    "**Action-value Bellman equation:**\n",
    "\n",
    "`Q(s,a) = Î£_s' P(s'|s,a) [R(s,a,s') + Î³Î£_a' Ï€(a'|s') Q(s',a')]`\n",
    "\n",
    "ðŸ’¡`Î£_a' Ï€(a'|s') Q(s',a')` from `V*(s')`\n",
    "\n",
    "where:\n",
    "\n",
    "- `Q(s,a)` is the expected cumulative reward of taking action `a` in state `s` under policy `Ï€`\n",
    "- `P(s'|s,a)` is the probability of transitioning to state `s'` after taking action `a` in state `s`\n",
    "- `R(s,a,s')` is the reward received for transitioning from state `s` to state `s'` after taking action `a`\n",
    "- `Î³` is the discount factor, which determines the relative importance of immediate and future rewards.\n",
    "- `Ï€(a'|s')` is the probability of taking action `a'` in state `s'` under policy `Ï€`\n",
    "\n",
    "This equation states that the value of taking action `a` in state `s` under policy `Ï€` is equal to the expected immediate reward of taking that action, plus the expected discounted future reward of transitioning to a successor state `s'` and taking the optimal action under `Ï€` in `s'`.\n",
    "\n",
    "Both forms of the Bellman equation are used in different reinforcement learning algorithms, such as value iteration, policy iteration, and Q-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701a44cf-6f72-4538-81d4-5a0e93f0b8f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
